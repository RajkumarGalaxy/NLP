{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "frequency_of_words.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN6od2rhyq1SJ69dVitd/9o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RajkumarGalaxy/rajkumar_nltk/blob/master/frequency_of_words.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KF_hmR2h1y1Q",
        "colab_type": "text"
      },
      "source": [
        "# Frequency of words in a document\n",
        "## A python NLP code\n",
        "### Using nltk api\n",
        "#### created in [colab](https://colab.research.google.com)\n",
        "created by [Rajkumar Lakshmanamoorthy](https://github.com/RajkumarGalaxy/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2FlegYBjsY9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import necessary libraries\n",
        "import nltk\n",
        "from nltk.probability import FreqDist\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pprint import pprint"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZCk_BRE2xhV",
        "colab_type": "text"
      },
      "source": [
        "### Download the corpora"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fi_GvzcXl7aR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "921500cb-ea16-4bdb-c4bd-670f4d686f84"
      },
      "source": [
        "nltk.download('gutenberg')"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sP7s3ADumIs6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c9b6a25c-258b-48dc-d701-ca585fb82966"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fGS8ipa3ex8",
        "colab_type": "text"
      },
      "source": [
        "### Fetch words from the corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2w0e-8DTj59_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c802fa8d-045f-4960-98f0-fa97dd15ae53"
      },
      "source": [
        "# read corpus and tokenize it\n",
        "words = nltk.Text(nltk.corpus.gutenberg.words('bryant-stories.txt'))\n",
        "stop = set(stopwords.words('english'))\n",
        "pprint(stop)\n",
        "print(len(stop))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'a',\n",
            " 'about',\n",
            " 'above',\n",
            " 'after',\n",
            " 'again',\n",
            " 'against',\n",
            " 'ain',\n",
            " 'all',\n",
            " 'am',\n",
            " 'an',\n",
            " 'and',\n",
            " 'any',\n",
            " 'are',\n",
            " 'aren',\n",
            " \"aren't\",\n",
            " 'as',\n",
            " 'at',\n",
            " 'be',\n",
            " 'because',\n",
            " 'been',\n",
            " 'before',\n",
            " 'being',\n",
            " 'below',\n",
            " 'between',\n",
            " 'both',\n",
            " 'but',\n",
            " 'by',\n",
            " 'can',\n",
            " 'couldn',\n",
            " \"couldn't\",\n",
            " 'd',\n",
            " 'did',\n",
            " 'didn',\n",
            " \"didn't\",\n",
            " 'do',\n",
            " 'does',\n",
            " 'doesn',\n",
            " \"doesn't\",\n",
            " 'doing',\n",
            " 'don',\n",
            " \"don't\",\n",
            " 'down',\n",
            " 'during',\n",
            " 'each',\n",
            " 'few',\n",
            " 'for',\n",
            " 'from',\n",
            " 'further',\n",
            " 'had',\n",
            " 'hadn',\n",
            " \"hadn't\",\n",
            " 'has',\n",
            " 'hasn',\n",
            " \"hasn't\",\n",
            " 'have',\n",
            " 'haven',\n",
            " \"haven't\",\n",
            " 'having',\n",
            " 'he',\n",
            " 'her',\n",
            " 'here',\n",
            " 'hers',\n",
            " 'herself',\n",
            " 'him',\n",
            " 'himself',\n",
            " 'his',\n",
            " 'how',\n",
            " 'i',\n",
            " 'if',\n",
            " 'in',\n",
            " 'into',\n",
            " 'is',\n",
            " 'isn',\n",
            " \"isn't\",\n",
            " 'it',\n",
            " \"it's\",\n",
            " 'its',\n",
            " 'itself',\n",
            " 'just',\n",
            " 'll',\n",
            " 'm',\n",
            " 'ma',\n",
            " 'me',\n",
            " 'mightn',\n",
            " \"mightn't\",\n",
            " 'more',\n",
            " 'most',\n",
            " 'mustn',\n",
            " \"mustn't\",\n",
            " 'my',\n",
            " 'myself',\n",
            " 'needn',\n",
            " \"needn't\",\n",
            " 'no',\n",
            " 'nor',\n",
            " 'not',\n",
            " 'now',\n",
            " 'o',\n",
            " 'of',\n",
            " 'off',\n",
            " 'on',\n",
            " 'once',\n",
            " 'only',\n",
            " 'or',\n",
            " 'other',\n",
            " 'our',\n",
            " 'ours',\n",
            " 'ourselves',\n",
            " 'out',\n",
            " 'over',\n",
            " 'own',\n",
            " 're',\n",
            " 's',\n",
            " 'same',\n",
            " 'shan',\n",
            " \"shan't\",\n",
            " 'she',\n",
            " \"she's\",\n",
            " 'should',\n",
            " \"should've\",\n",
            " 'shouldn',\n",
            " \"shouldn't\",\n",
            " 'so',\n",
            " 'some',\n",
            " 'such',\n",
            " 't',\n",
            " 'than',\n",
            " 'that',\n",
            " \"that'll\",\n",
            " 'the',\n",
            " 'their',\n",
            " 'theirs',\n",
            " 'them',\n",
            " 'themselves',\n",
            " 'then',\n",
            " 'there',\n",
            " 'these',\n",
            " 'they',\n",
            " 'this',\n",
            " 'those',\n",
            " 'through',\n",
            " 'to',\n",
            " 'too',\n",
            " 'under',\n",
            " 'until',\n",
            " 'up',\n",
            " 've',\n",
            " 'very',\n",
            " 'was',\n",
            " 'wasn',\n",
            " \"wasn't\",\n",
            " 'we',\n",
            " 'were',\n",
            " 'weren',\n",
            " \"weren't\",\n",
            " 'what',\n",
            " 'when',\n",
            " 'where',\n",
            " 'which',\n",
            " 'while',\n",
            " 'who',\n",
            " 'whom',\n",
            " 'why',\n",
            " 'will',\n",
            " 'with',\n",
            " 'won',\n",
            " \"won't\",\n",
            " 'wouldn',\n",
            " \"wouldn't\",\n",
            " 'y',\n",
            " 'you',\n",
            " \"you'd\",\n",
            " \"you'll\",\n",
            " \"you're\",\n",
            " \"you've\",\n",
            " 'your',\n",
            " 'yours',\n",
            " 'yourself',\n",
            " 'yourselves'}\n",
            "179\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xivKGy2MmTaN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = [i.lower() for i in words if i.isalpha()]\n",
        "words = [i.lower() for i in words if i not in stop]"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vigGs9mUoN9m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "02ba157d-ebbb-4d86-ed92-8460f64ea55d"
      },
      "source": [
        "print('Total number of words: ', len(words))\n",
        "print('Vocabulary count:      ', len(set(words)))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of words:  21718\n",
            "Vocabulary count:       3688\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLjiLzZf3rnm",
        "colab_type": "text"
      },
      "source": [
        "### Find most frequent words in the document"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xd7o5qRGo0ye",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        },
        "outputId": "dd3046c5-3eac-4361-91d8-e2d88cb69348"
      },
      "source": [
        "fdist = FreqDist(words)\n",
        "top = 50\n",
        "table = pd.DataFrame(index=np.arange(1,top+1,1), columns=['word', 'frequency', 'normalized_frequency', 'count_document_ratio'])\n",
        "i = 1\n",
        "print('frequency            --- count of the word')\n",
        "print('normalized_frequency --- ratio of frequency to vocabulary count')\n",
        "print('count_document_ratio --- ratio of frequency to total count of words in document')\n",
        "print('-'*80)\n",
        "for x, v in fdist.most_common(top):\n",
        "  table.loc[i, ['word', 'frequency', 'normalized_frequency', 'count_document_ratio']] = x, v, v/len(fdist), v/len(words)\n",
        "  i += 1\n",
        "print(table)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "frequency            --- count of the word\n",
            "normalized_frequency --- ratio of frequency to vocabulary count\n",
            "count_document_ratio --- ratio of frequency to total count of words in document\n",
            "--------------------------------------------------------------------------------\n",
            "           word frequency normalized_frequency count_document_ratio\n",
            "1        little       597             0.161876            0.0274887\n",
            "2          said       453             0.122831            0.0208583\n",
            "3          came       191            0.0517896           0.00879455\n",
            "4           one       183            0.0496204           0.00842619\n",
            "5         could       158            0.0428416           0.00727507\n",
            "6          king       141            0.0382321           0.00649231\n",
            "7          went       122            0.0330803           0.00561746\n",
            "8         would       112            0.0303688           0.00515701\n",
            "9         great       110            0.0298265           0.00506492\n",
            "10          day       107             0.029013           0.00492679\n",
            "11          man       107             0.029013           0.00492679\n",
            "12          old       104            0.0281996           0.00478865\n",
            "13         time        97            0.0263015           0.00446634\n",
            "14          see        97            0.0263015           0.00446634\n",
            "15         like        92            0.0249458           0.00423612\n",
            "16          saw        92            0.0249458           0.00423612\n",
            "17         away        91            0.0246746           0.00419007\n",
            "18       mother        90            0.0244035           0.00414403\n",
            "19         made        89            0.0241323           0.00409798\n",
            "20         good        84            0.0227766           0.00386776\n",
            "21         come        83            0.0225054           0.00382171\n",
            "22       jackal        80             0.021692           0.00368358\n",
            "23           go        77            0.0208785           0.00354545\n",
            "24       father        77            0.0208785           0.00354545\n",
            "25       people        76            0.0206074            0.0034994\n",
            "26       looked        76            0.0206074            0.0034994\n",
            "27      margery        73            0.0197939           0.00336127\n",
            "28          boy        71            0.0192516           0.00326918\n",
            "29          ran        69            0.0187093           0.00317709\n",
            "30          big        69            0.0187093           0.00317709\n",
            "31         make        67             0.018167             0.003085\n",
            "32         know        66            0.0178959           0.00303895\n",
            "33      thought        65            0.0176247           0.00299291\n",
            "34          two        64            0.0173536           0.00294686\n",
            "35         home        63            0.0170824           0.00290082\n",
            "36        every        62            0.0168113           0.00285477\n",
            "37          put        61            0.0165401           0.00280873\n",
            "38         lion        59            0.0159978           0.00271664\n",
            "39        mouse        59            0.0159978           0.00271664\n",
            "40         long        58            0.0157267            0.0026706\n",
            "41        never        58            0.0157267            0.0026706\n",
            "42         took        57            0.0154555           0.00262455\n",
            "43          way        57            0.0154555           0.00262455\n",
            "44         much        57            0.0154555           0.00262455\n",
            "45         back        56            0.0151844           0.00257851\n",
            "46         last        55            0.0149132           0.00253246\n",
            "47           oh        53            0.0143709           0.00244037\n",
            "48  nightingale        53            0.0143709           0.00244037\n",
            "49         door        52            0.0140998           0.00239433\n",
            "50        began        52            0.0140998           0.00239433\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOzM03LF37fD",
        "colab_type": "text"
      },
      "source": [
        "#### Let's meet with some advanced techniques later"
      ]
    }
  ]
}